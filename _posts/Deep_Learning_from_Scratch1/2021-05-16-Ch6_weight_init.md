---
layout: single
title:  "Deep Learning from Scratch1 - Ch6 가중치 초깃값"
excerpt: "가중치 초깃값에 따른 활성화값 분포"

categories:
  - Deep Learning from Scratch1
# tags:
#   - [ML, Python]

toc: true
toc_sticky: true
---

# 가중치의 초깃값
- 가중치의 초깃값을 무엇으로 설정하느냐가 신경망 학습의 성패에 큰 영향을 끼칠 수 있다.

## 시그모이드(sigmoid)를 사용할 때의 가중치 초깃값
- 활성화 함수로 시그모이드 함수를 사용하는 5층 신경망에 무작위로 생성한 입력 데이터를 흘리며 각 층의 활성화값 분포를 히스토그램으로 관찰

### 표준편차가 1인 정규분포로 가중치 초기화

<img src="https://user-images.githubusercontent.com/59792046/118388431-648c5980-b65f-11eb-93b1-ba8d19a1c36f.jpg" width="600">

- 시그모이드 함수는 출력값이 0 또는 1에 가까워지면 미분값은 0에 다가간다. 따라서 각 층의 활성화값들이 0과 1에 치우치면 역전파의 기울기 값이 점점 작아지다가 사라진다.
- 이런 '기울기 소실(gradient vanishing)' 문제는 층을 깊게 하는 딥러닝에서 더욱 심각한 문제가 될 수 있다.

### 표준편차가 0.01인 정규분포로 가중치 초기화

<img src="https://user-images.githubusercontent.com/59792046/118388433-65bd8680-b65f-11eb-95f7-6c9276b40355.jpg" width="600">

- 활성화값이 특정값에 치우쳤다는 것은 표현력 관점에서는 큰 문제가 있는 것이다.
- 다수의 뉴런이 거의 같은 값을 출력하고 있으니 뉴런을 여러 개 둔 의미가 없어진다. 
- 각 층의 활성화값들은 적당히 고루 분포되어야 한다. 층과 층 사이에 적당하게 다양한 데이터가 흐르게 해야 신경망 학습이 효율적으로 이루어질 수 있다.
- 치우친 데이터가 흐르면 기울기 소실이나 표현력 제한 문제에 빠질 수 있다.

### Xavier 초깃값으로 가중치 초기화
- 사비에르 글로로트(Xavier Glorot)와 요슈아 벤지오(Yoshua Bengio)의 논문에서 권장하는 가중치 초깃값이다.
- 앞 계층의 노드가 n개라면 표준편차가 1/sqrt(n) 인 분포를 사용하면 된다.
- 앞 층에 노드가 많을수록 대상 노드의 초깃값으로 설정하는 가중치가 좁게 퍼진다. 
- 활성화 함수가 선형인 것을 전제로 이끈 결과이다. sigmoid 함수와 tanh 함수는 좌우 대칭이라 중앙 부근이 선형인 함수로 볼 수 있다. 

<img src="https://user-images.githubusercontent.com/59792046/118388947-50962700-b662-11eb-8271-f14e663e39b3.jpg" width="600">

- 층이 깊어지면서 형태가 다소 일그러지지만, 앞에서 본 방식보다는 확실히 넓게 분포되어 있음을 확인할 수 있다.
- 각 층에 흐르는 데이터는 적당히 퍼져 있으므로, 시그모이드 함수의 표현력도 제한받지 않고 학습이 효율적으로 이루어질 수 있을 것으로 기대된다.

## ReLU를 사용할 때의 가중치 초깃값
- ReLU의 경우 특화된 초깃값을 이용할 것을 권장한다.

### He 초깃값
- 앞 계층의 노드가 n개일 때, 표준편차가 sqrt(2/n)인 정규분포를 사용한다.
- ReLU는 음의 영역이 0이라서 더 넓게 분포시키기 위해 2배의 계수가 필요하다고 해석할 수 있다.

### ReLU에 초깃값을 다르게 적용한 활성화값 분포

<img src="https://user-images.githubusercontent.com/59792046/118389164-7ff96380-b663-11eb-9742-a050e7b97cea.jpg" width="600">

- 표준편차가 0.01일 때 각 층의 활성화값들은 아주 작은 값들이다. 신경망에 아주 작은 데이터가 흐른다는 것은 역전파 때 가중치의 기울기 역시 작아진다는 것을 의미한다. 실제로 학습이 거의 이루어지지 않을 것이다.
- Xavier 초깃값 결과를 보면 층이 깊어질수록 치우침이 점점 커진다. 실제로도 층이 깊어지면 활성화값들의 치우침도 커지고, 결국 '기울기 소실'문제를 일으킨다.
- He 초깃값은 모든 층에서 균일하게 분포되어 있다. 층이 깊어져도 균일한 분포를 유지하므로 역전파 때도 적절한 값이 나올 것으로 예상된다. 

## MNIST 데이터셋으로 본 가중치 초깃값 비교
- 층별 뉴런수가 100개인 5층 신경망에서 활성화 함수로 ReLU를 사용

<img src="https://user-images.githubusercontent.com/59792046/118389287-31989480-b664-11eb-91eb-2adc67c47ca1.png" width="600">

- std=0.01일 때 전혀 학습이 이루어지지 않고 있다. 순전파 때 너무 작은 값이 흐르기 때문에 역전파 때의 기울기도 작아져 가중치가 거의 갱신되지 않는 것이다.
- Xavier와 He 초깃값은 순조롭게 학습이 진행되고 있다. 학습속도는 He 초깃값이 더 빠른 것으로 확인된다.

#### [참고자료]
- 밑바닥부터 시작하는 딥러닝
